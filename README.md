## ğŸš€ CycleTrap

This is the code repository for our paper:  ```CycleTrap: Low-Entropy Loop Induction for Energy-Latency Attacks on Multimodal Large Language Models```

## ğŸ“ Abstract

<abstract>

![overview](https://github.com/neuron-insight-lab/CycleTrap/raw/main/assets/overview.png)

## ğŸ› ï¸ Installation 

### ğŸ”§ Environment Preparation

```bash
conda env create -f env.yaml
conda activate CycleTrap
```

### ğŸ”¨ Model Preparation

You can download the required MLLMs from the [huggingface](https://huggingface.co/)  (such as [Qwen2.5-VL-7B-Instrcut](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)) and fill in your model path in the corresponding position of the ```utils/vision_utils.py``` file.

### ğŸ“Š Dataset Preparation

We selected four datasets: [MS-COCO](https://cocodataset.org/#download), [ImageNet](https://image-net.org/download-images.php), [TGIF](https://github.com/raingo/TGIF-Release) and [MSVD](https://github.com/jpthu17/EMCL), and chose 200 image and 100 video as the evaluation dataset. After downloading, fill in your dataset paths into the corresponding position of the ```utils/opt_utils.py``` file.


## ğŸ’¡ Usage

Run the following bash file to complete "Main Experiment" in our paper 

```bash
./main.sh
```

Or you can run the following command to create adversarial vision input generated by CycleTrap to induce Qwen2.5-VL-7B-Instruct into repetitive generation.

```bash
python main.py \
      --model_name "Qwen2.5-VL-7B" \
      --data_name "coco" \
      --data_len 200    \
      --segment_len 5 \
      --max_new_tokens 1024 \
      --steps 300 \
      --root_dir "save/CycleTrap" \
```