## 🚀 Introduction

This is the code repository for our paper:  ``CycleTrap: Low-Entropy Loop Induction for Energy-Latency Attacks on Multimodal Large Language Models``


## 📂 Directory Structure

```
.
├── assets/
│   └── overview.png
├── baseline/
│   ├── LingoLoop.py
│   ├── README.md
│   ├── VerboseVideo.py
│   └── VerboseImage.py
├── utils/
│   ├── __init__.py
│   ├── opt_utils.py
│   └── vision_utils.py
├── README.md
├── env.yaml
├── main.py
└── main.sh
```

## 🎓 Abstract

Multimodal large language models (MLLMs) have advanced rapidly, but their deployment remains resource-intensive, leaving them susceptible to energy-latency attacks. Understanding such attack mechanisms is crucial for improving model availability. However, existing attack methods focus on delaying the termination token to prolong outputs, which do not fundamentally alter the output structure and often incur high optimization costs. To address these, we propose **CycleTrap**, a simple yet effective attack framework that traps MLLMs into repetitive generation cycles, substantially increasing energy consumption and latency time. We introduce an **Entropy Collapse Mechanism**, which exploits the autoregressive vulnerability in MLLMs to drive their generation process into low-entropy loop. Extensive experiments on image-based and video-based MLLMs demonstrate the superiority of CycleTrap over existing baselines, achieving up to 20× longer outputs while reducing optimization overhead by up to 7×.

![overview](https://github.com/neuron-insight-lab/CycleTrap/raw/main/assets/overview.png)


## 🛠️ Installation 

### 🔧 Environment Preparation

```bash
conda env create -f env.yaml
conda activate CycleTrap
```

### 🔨 Model Preparation

You can download the required MLLMs from the [huggingface](https://huggingface.co/), such as [Qwen2.5-VL-7B-Instrcut](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct), [InternVL3-8B](https://huggingface.co/OpenGVLab/InternVL3-8B), [InstructBLIP](https://huggingface.co/Salesforce/instructblip-vicuna-7b) ([-Video](https://huggingface.co/docs/transformers/v4.55.4/en/model_doc/instructblipvideo)) and [LLaVA-NeXT](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf) ([-Video](https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf)), and fill in your model path in the corresponding position of the [utils/vision_utils.py](https://github.com/neuron-insight-lab/CycleTrap/blob/main/utils/vision_utils.py#L19) file.

### 📊 Dataset Preparation

We selected four datasets: [MS-COCO](https://cocodataset.org/#download), [ImageNet](https://image-net.org/download-images.php), [TGIF](https://github.com/raingo/TGIF-Release) and [MSVD](https://github.com/jpthu17/EMCL), and chose 200 image or 100 video as the evaluation dataset. After downloading, fill in your dataset paths into the corresponding position of the [utils/opt_utils.py](https://github.com/neuron-insight-lab/CycleTrap/blob/main/utils/opt_utils.py#L21) file.


## 💡 Quick Start

Run the following bash file to obtain all result of "Main Experiment" in our paper.

```bash
./main.sh
```

Or you can run the following command to create adversarial vision inputs generated by CycleTrap to induce ``Qwen2.5-VL-7B-Instruct`` into repetitive generation.

```bash
python main.py \
      --model_name "Qwen2.5-VL-7B" \
      --data_name "coco" \
      --data_len 200    \
      --segment_len 5 \
      --max_new_tokens 1024 \
      --steps 300 \
      --root_dir "save/CycleTrap"
```

You can run the ``python main.py -h`` command to understand the meaning of each parameter.